{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Fake News Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import cleantext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task 1:\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "def clean_content(text):\n",
    "    cleaned_text = cleantext.clean(text,\n",
    "                                   lower=True,\n",
    "                                   no_urls=True,\n",
    "                                   no_emails=True,\n",
    "                                   no_phone_numbers=True,\n",
    "                                   no_numbers=True,\n",
    "                                   no_digits=True,\n",
    "                                   no_currency_symbols=True,\n",
    "                                   no_punct=True,\n",
    "                                   replace_with_url=\"<URL>\",\n",
    "                                   replace_with_email=\"<EMAIL>\",\n",
    "                                   replace_with_phone_number=\"<PHONE>\",\n",
    "                                   replace_with_number=\"<NUM>\",\n",
    "                                   replace_with_digit=\"<NUM>\",\n",
    "                                   replace_with_currency_symbol=\"<CUR>\")\n",
    "    return cleaned_text\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_content)\n",
    "\n",
    "# Tokenize the text\n",
    "df['tokens'] = df['clean_content'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens_no_stopwords'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# Stemming\n",
    "ss = SnowballStemmer('english')\n",
    "df['tokens_stemmed'] = df['tokens_no_stopwords'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the vocabulary size before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vocabulary_size(tokens_list):\n",
    "    unique_words = set()\n",
    "    for tokens in tokens_list:\n",
    "        unique_words.update(tokens)\n",
    "    return len(unique_words)\n",
    "\n",
    "# Compute vocabulary size before removing stopwords\n",
    "original_vocabulary_size = compute_vocabulary_size(df['tokens'])\n",
    "print(f\"Original vocabulary size: {original_vocabulary_size}\")\n",
    "\n",
    "# Compute vocabulary size after removing stopwords\n",
    "reduced_vocabulary_size_no_stopwords = compute_vocabulary_size(df['tokens_no_stopwords'])\n",
    "print(f\"Vocabulary size after removing stopwords: {reduced_vocabulary_size_no_stopwords}\")\n",
    "\n",
    "# Compute reduction rate of the vocabulary size after removing stopwords\n",
    "reduction_rate_no_stopwords = (original_vocabulary_size - reduced_vocabulary_size_no_stopwords) / original_vocabulary_size * 100\n",
    "print(f\"Reduction rate after removing stopwords: {reduction_rate_no_stopwords:.2f}%\")\n",
    "\n",
    "# Compute vocabulary size after stemming\n",
    "reduced_vocabulary_size_stemmed = compute_vocabulary_size(df['tokens_stemmed'])\n",
    "print(f\"Vocabulary size after stemming: {reduced_vocabulary_size_stemmed}\")\n",
    "\n",
    "# Compute reduction rate of the vocabulary size after stemming\n",
    "reduction_rate_stemmed = (original_vocabulary_size - reduced_vocabulary_size_stemmed) / original_vocabulary_size * 100\n",
    "print(f\"Reduction rate after stemming: {reduction_rate_stemmed:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def count_elements(row):\n",
    "    # Combines all text columns\n",
    "    combined_text = ' '.join(row.astype(str))\n",
    "\n",
    "    # Here we count the URL's\n",
    "    url_pattern = r'(?:http[s]?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' \n",
    "    urls = re.findall(url_pattern, combined_text)\n",
    "    url_count = len(urls)\n",
    "\n",
    "    # Here we count dates\n",
    "    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    dates = re.findall(date_pattern, combined_text)\n",
    "    date_count = len(dates)\n",
    "\n",
    "    # Here we count the numeric values\n",
    "    numeric_pattern = r'\\d'\n",
    "    numeric_values = re.findall(numeric_pattern, combined_text)\n",
    "    numeric_count = len(numeric_values)\n",
    "\n",
    "    return url_count, date_count, numeric_count\n",
    "\n",
    "# Here we save the results to a new column\n",
    "df['url_count'], df['date_count'], df['numeric_count'] = zip(*df.apply(count_elements, axis=1))\n",
    "\n",
    "# Here we save sum up the number for each of the columns\n",
    "total_urls = df['url_count'].sum()\n",
    "total_dates = df['date_count'].sum()\n",
    "total_numeric_values = df['numeric_count'].sum()\n",
    "\n",
    "print(f\"Total number of URLs: {total_urls}\")\n",
    "print(f\"Total number of dates: {total_dates}\")\n",
    "print(f\"Total number of numeric values: {total_numeric_values}\")\n",
    "\n",
    "# Here we count the frequency of the top 20 authors\n",
    "author_counts = df['authors'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "author_counts.head(20).plot(kind='bar')\n",
    "plt.xlabel(\"Authors\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 authors\")\n",
    "plt.show()\n",
    "\n",
    "# Here we calculate the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "sorted_missing_values = missing_values.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sorted_missing_values.head(6).plot(kind='bar') #We started off by plotting for each column, but only the first 5 (in the sorted) are interesting as the others are 0.\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Number of missing values\")\n",
    "plt.title(\"Missing values in each column\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "def count_word_frequencies(df, ignore_words=None):\n",
    "    word_freq = {}\n",
    "    for tokens in df:\n",
    "        for word in tokens:\n",
    "            if ignore_words and word.lower() in ignore_words:\n",
    "                continue\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    items = list(word_freq.items())\n",
    "    items.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    top_word_freq = items[:10000]\n",
    "    return top_word_freq\n",
    "\n",
    "# Before removing stopwords and applying stemming\n",
    "top10000_before = count_word_frequencies(df['tokens'])\n",
    "\n",
    "# After removing stopwords and applying stemming\n",
    "top10000_after = count_word_frequencies(df['tokens_stemmed'], ignore_words=['url', 'num', 'email', 'phone', 'cur', '<', '>'])\n",
    "\n",
    "words_before = [word[0] for word in top10000_before]\n",
    "freq_before = [freq[1] for freq in top10000_before]\n",
    "\n",
    "words_after = [word[0] for word in top10000_after]\n",
    "freq_after = [freq[1] for freq in top10000_after]\n",
    "\n",
    "# Here we plot the top 10,000 most frequent words before and after preprocessing\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "ax1.plot(freq_before)\n",
    "ax1.set_xlabel(\"Word Index\")\n",
    "ax1.set_ylabel(\"Frequencies\")\n",
    "ax1.set_title(\"Before preprocessing\")\n",
    "\n",
    "ax2.plot(freq_after)\n",
    "ax2.set_xlabel(\"Word Index\")\n",
    "ax2.set_ylabel(\"Frequencies\")\n",
    "ax2.set_title(\"After preprocessing\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics and visualizations\n",
    "\n",
    "# Distribution of domains\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "domain_counts.plot(kind='bar')\n",
    "plt.xlabel(\"Domains\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Domain frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Here we calculate the average number of tokens before and after preprocessing\n",
    "df['content_length_before'] = df['tokens'].apply(len)\n",
    "df['content_length_after'] = df['tokens_stemmed'].apply(len)\n",
    "\n",
    "avg_before = np.mean(df['content_length_before'])\n",
    "avg_after = np.mean(df['content_length_after'])\n",
    "\n",
    "print(f\"Average number of tokens per article before preprocessing: {avg_before:.2f}\")\n",
    "print(f\"Average number of tokens per article after preprocessing: {avg_after:.2f}\")\n",
    "\n",
    "# Here we create a bar chart to visualize the difference\n",
    "plt.bar(['Before preprocessing', 'After preprocessing'], [avg_before, avg_after])\n",
    "plt.ylabel(\"Average number of tokens\")\n",
    "plt.title(\"Average content length before and after preprocessing\")\n",
    "plt.show()\n",
    "\n",
    "# Here we count number of articles in each category\n",
    "type_counts = df['type'].value_counts()\n",
    "plt.figure(figsize=(12, 6))\n",
    "type_counts.plot(kind='bar')\n",
    "plt.xlabel(\"Article type\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of articles by type\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and saving to a new CSV-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Task 3. For this task we only manipulate the 'content' column. We overwrite this, as mentioned in the report, due to the fact that we do not need the original 'content' column for anything\n",
    "# data = pd.read_csv('news_sample.csv', engine=\"python\")\n",
    "\n",
    "# # Here we apply our clean_content function defined in task 1\n",
    "# data['content'] = data['content'].apply(clean_content)\n",
    "\n",
    "# # Here we tokenize our text\n",
    "# data['content'] = data['content'].apply(word_tokenize)\n",
    "\n",
    "# # Here we remove the stopwords\n",
    "# data['content'] = data['content'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "\n",
    "# # Here we apply stemming \n",
    "# data['content'] = data['content'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "# # Here we save the resulting dataframe to a new csv file, which we use throughout the assignment\n",
    "# data.to_csv('news_sample_cleaned2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = pd.read_csv('news_sample_cleaned2.csv', nrows=100000)\n",
    "\n",
    "df_large.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1, task 4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This is part of part 2 task 0\n",
    "df_large['label'] = df_large['type'].apply(lambda x: 'reliable' if x in ['reliable', 'political'] else 'fake')\n",
    "\n",
    "# This is part of part 2 task 1\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_large['content'])\n",
    "y = df_large['label']\n",
    "\n",
    "# Here we split the dataset into training, validation, and test sets. We, as mentioned, complete some of part 2 in this task, as we thought it was obvious to do here.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Here we define our simple models that we want to use for this task\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, C=10, penalty='l2')),\n",
    "    ('Naive Bayes', MultinomialNB(alpha=0.1)),\n",
    "    ('Support Vector Machine', LinearSVC(max_iter=1000, C=10, penalty='l2'))\n",
    "]\n",
    " \n",
    "# Here we calculate the accuracy of each model using accuracy_score\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Here we define the range of hyperparameters to search over when running GridSearch\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Here we create a grid search object and fit it to the training data\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "# Here we define the hyperparameters for the Naive Bayes classifier\n",
    "nb_params = {'alpha': [0.1, 0.5, 1.0]}\n",
    "\n",
    "# Here we create a Naive Bayes classifier with the hyperparameters\n",
    "nb = MultinomialNB()\n",
    "nb_grid_search = GridSearchCV(nb, nb_params, cv=5)\n",
    "nb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Naive Bayes best hyperparameters:\", nb_grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", nb_grid_search.best_score_)\n",
    "\n",
    "# We define some relevant parameters for the Support Vector Machine\n",
    "svm_params = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "svm = LinearSVC(max_iter=1000)\n",
    "svm_grid_search = GridSearchCV(svm, svm_params, cv=5)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"SVM best hyperparameters:\", svm_grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", svm_grid_search.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing our titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we clean the titles using the function from task 1 \n",
    "df_large['title'] = df_large['title'].apply(clean_content)\n",
    "\n",
    "# Here we tokenize the titles\n",
    "df_large['title'] = df_large['title'].apply(word_tokenize)\n",
    "\n",
    "# Here we remove stopwords from the titles\n",
    "df_large['title'] = df_large['title'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# Here we stem the titles\n",
    "df_large['title'] = df_large['title'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "# Here we save the resulting df to a new csv-file, which we use later on\n",
    "df_large['title'].to_csv('processed_titles.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our preprocessed titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_titles = pd.read_csv('processed_titles.csv', nrows=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including meta-data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Here we have defined different meta-features. We tested it with each of them, but due to possible overfitting, we went with the keywords. \n",
    "# X_title = vectorizer.fit_transform(df_large_titles['title'])\n",
    "# X_domain = vectorizer.fit_transform(df_large['domain'])\n",
    "X_keywords = vectorizer.fit_transform(df_large['meta_keywords'])\n",
    "\n",
    "# Here we combine the content with the meta-data feature(s)\n",
    "X_combined = hstack([X, X_keywords])\n",
    "\n",
    "# Here we create a new test-set based on the combination above\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "X_val_combined, X_test_combined, y_val_combined, y_test_combined = train_test_split(X_test_combined, y_test_combined, test_size=0.5, random_state=42)\n",
    "\n",
    "# Here we calculate the accuracy of each model using accuracy_score\n",
    "for name, model in models:\n",
    "    model.fit(X_train_combined, y_train_combined)\n",
    "    y_pred_combined = model.predict(X_val_combined)\n",
    "    accuracy_combined = accuracy_score(y_val_combined, y_pred_combined)\n",
    "    print(f\"{name} accuracy (combined features): {accuracy_combined:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "# Our own custom function for calculation f1\n",
    "def f1_score_custom(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Here we encode the labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Here we split the dataset into training, validation, and test sets for the neural network\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "X_val_nn, X_test_nn, y_val_nn, y_test_nn = train_test_split(X_test_nn, y_test_nn, test_size=0.5, random_state=42)\n",
    "\n",
    "# Here we create our neural network and add to it\n",
    "advancedmodel = Sequential()\n",
    "advancedmodel.add(Dense(128, activation='relu'))\n",
    "advancedmodel.add(Dropout(0.5))\n",
    "advancedmodel.add(Dense(64, activation='relu'))\n",
    "advancedmodel.add(Dropout(0.5))\n",
    "advancedmodel.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Here we complie our model with parameters\n",
    "advancedmodel.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy', f1_score_custom])\n",
    "\n",
    "\n",
    "# Here we train the model\n",
    "history = advancedmodel.fit(X_train_nn.toarray(), y_train_nn, validation_data=(X_val_nn.toarray(), y_val_nn), epochs=10, batch_size=32)\n",
    "\n",
    "# Here we test the model\n",
    "nnscore = advancedmodel.evaluate(X_val_nn.toarray(), y_val_nn, batch_size=32)\n",
    "print(f\"Neural network accuracy: {nnscore[1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing F1-scores, accuracy and confusion matrices for the simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# We show the confusion matrix and f1-score for each simple model\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} F1-score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the test-sets for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we use the test-set specifically for the neural network model\n",
    "nntestscore = advancedmodel.evaluate(X_test_nn.toarray(), y_test_nn, batch_size=32)\n",
    "print(f\"Neural network:\\nAccuracy: {nntestscore[1]:.4f} \\nF1 score: {nntestscore[2]:.4f} \")\n",
    "\n",
    "# We compute the accuracy and f1_score for each of the simple models\n",
    "for name, model1 in models:\n",
    "    model1.fit(X_train, y_train)\n",
    "    y_pred_test = model1.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    f1score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    print(f\"\\n{name}: \\nAccuracy: {accuracy:.4f} \\nF1 score: {f1score:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our models directly on the LIAR-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "liar_data_test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "liar_data_test.columns = ['ID', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count', 'context']\n",
    "\n",
    "liar_data_train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "liar_data_train.columns = ['ID', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count', 'context']\n",
    "\n",
    "liar_data_valid = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "liar_data_valid.columns = ['ID', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count', 'context']\n",
    "\n",
    "X_train_liar = vectorizer.transform(liar_data_train['statement'])\n",
    "y_train_liar = liar_data_train['label'].apply(lambda x: 'reliable' if x in ['true', 'mostly-true', 'half-true'] else 'fake')\n",
    "\n",
    "X_test_liar = vectorizer.transform(liar_data_test['statement'])\n",
    "y_test_liar = liar_data_test['label'].apply(lambda x: 'reliable' if x in ['true', 'mostly-true', 'half-true'] else 'fake')\n",
    "\n",
    "\n",
    "# This tests our simple models on the LIAR-dataset\n",
    "for name, model in models:\n",
    "    model.fit(X_train_liar, y_train_liar)\n",
    "    y_pred_liar = model.predict(X_test_liar)\n",
    "    accuracy = accuracy_score(y_test_liar, y_pred_liar)\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "y_test_liar_encoded = le.fit_transform(y_test_liar)\n",
    "y_test_liar_categorical = to_categorical(y_test_liar_encoded)\n",
    "\n",
    "# THESE LINES CAUSES ERRORS, which means that we could not test the advanced model on the LIAR data-set (mentioned in report aswell)\n",
    "\n",
    "# nntestscoreliar = advancedmodel.evaluate(X_test_liar.toarray(), y_test_liar_categorical, batch_size=32)\n",
    "# print(f\"Neural network accuracy: {nntestscore[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4693ac305c968c1fc088116fd30aaabd46d27a6daef9db092bc78234e9d76555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
