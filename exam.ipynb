{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 1 - Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import cleantext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task 1:\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "def clean_content(text):\n",
    "    cleaned_text = cleantext.clean(text,\n",
    "                                   lower=True,\n",
    "                                   no_urls=True,\n",
    "                                   no_emails=True,\n",
    "                                   no_phone_numbers=True,\n",
    "                                   no_numbers=True,\n",
    "                                   no_digits=True,\n",
    "                                   no_currency_symbols=True,\n",
    "                                   no_punct=True,\n",
    "                                   replace_with_url=\"<URL>\",\n",
    "                                   replace_with_email=\"<EMAIL>\",\n",
    "                                   replace_with_phone_number=\"<PHONE>\",\n",
    "                                   replace_with_number=\"<NUM>\",\n",
    "                                   replace_with_digit=\"<NUM>\",\n",
    "                                   replace_with_currency_symbol=\"<CUR>\")\n",
    "    return cleaned_text\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_content)\n",
    "\n",
    "# Tokenize the text\n",
    "df['tokens'] = df['clean_content'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens_no_stopwords'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# Stemming\n",
    "ss = SnowballStemmer('english')\n",
    "df['tokens_stemmed'] = df['tokens_no_stopwords'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "def compute_vocabulary_size(tokens_list):\n",
    "    unique_words = set()\n",
    "    for tokens in tokens_list:\n",
    "        unique_words.update(tokens)\n",
    "    return len(unique_words)\n",
    "\n",
    "# Compute vocabulary size before removing stopwords\n",
    "original_vocabulary_size = compute_vocabulary_size(df['tokens'])\n",
    "print(f\"Original vocabulary size: {original_vocabulary_size}\")\n",
    "\n",
    "# Compute vocabulary size after removing stopwords\n",
    "reduced_vocabulary_size_no_stopwords = compute_vocabulary_size(df['tokens_no_stopwords'])\n",
    "print(f\"Vocabulary size after removing stopwords: {reduced_vocabulary_size_no_stopwords}\")\n",
    "\n",
    "# Compute reduction rate of the vocabulary size after removing stopwords\n",
    "reduction_rate_no_stopwords = (original_vocabulary_size - reduced_vocabulary_size_no_stopwords) / original_vocabulary_size * 100\n",
    "print(f\"Reduction rate after removing stopwords: {reduction_rate_no_stopwords:.2f}%\")\n",
    "\n",
    "# Compute vocabulary size after stemming\n",
    "reduced_vocabulary_size_stemmed = compute_vocabulary_size(df['tokens_stemmed'])\n",
    "print(f\"Vocabulary size after stemming: {reduced_vocabulary_size_stemmed}\")\n",
    "\n",
    "# Compute reduction rate of the vocabulary size after stemming\n",
    "reduction_rate_stemmed = (original_vocabulary_size - reduced_vocabulary_size_stemmed) / original_vocabulary_size * 100\n",
    "print(f\"Reduction rate after stemming: {reduction_rate_stemmed:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: \n",
    "import re\n",
    "import numpy as np\n",
    "def count_elements(row):\n",
    "    # Combine all text columns\n",
    "    combined_text = ' '.join(row.astype(str))\n",
    "\n",
    "    # Counts URLs\n",
    "    url_pattern = r'(?:http[s]?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' \n",
    "    urls = re.findall(url_pattern, combined_text)\n",
    "    url_count = len(urls)\n",
    "\n",
    "    # Counts dates\n",
    "    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    dates = re.findall(date_pattern, combined_text)\n",
    "    date_count = len(dates)\n",
    "\n",
    "    # Counts numeric values\n",
    "    numeric_pattern = r'\\d'\n",
    "    numeric_values = re.findall(numeric_pattern, combined_text)\n",
    "    numeric_count = len(numeric_values)\n",
    "\n",
    "    return url_count, date_count, numeric_count\n",
    "\n",
    "df['url_count'], df['date_count'], df['numeric_count'] = zip(*df.apply(count_elements, axis=1))\n",
    "\n",
    "total_urls = df['url_count'].sum()\n",
    "total_dates = df['date_count'].sum()\n",
    "total_numeric_values = df['numeric_count'].sum()\n",
    "\n",
    "print(f\"Total number of URLs: {total_urls}\")\n",
    "print(f\"Total number of dates: {total_dates}\")\n",
    "print(f\"Total number of numeric values: {total_numeric_values}\")\n",
    "\n",
    "\n",
    "# Counts the frequency of the top 20 authors\n",
    "author_counts = df['authors'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "author_counts.head(20).plot(kind='bar')\n",
    "plt.xlabel(\"Authors\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 authors\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "sorted_missing_values = missing_values.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sorted_missing_values.head(6).plot(kind='bar') #We started off by plotting for each column, but only the first 5 (in the sorted) are interesting as the others are 0.\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Number of missing values\")\n",
    "plt.title(\"Missing values in each column\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "def count_word_frequencies(df, ignore_words=None):\n",
    "    word_freq = {}\n",
    "    for tokens in df:\n",
    "        for word in tokens:\n",
    "            if ignore_words and word.lower() in ignore_words:\n",
    "                continue\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    items = list(word_freq.items())\n",
    "    items.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    top_word_freq = items[:10000]\n",
    "    return top_word_freq\n",
    "\n",
    "# Before removing stopwords and applying stemming\n",
    "top10000_before = count_word_frequencies(df['tokens'])\n",
    "\n",
    "# After removing stopwords and applying stemming\n",
    "top10000_after = count_word_frequencies(df['tokens_stemmed'], ignore_words=['url', 'num', 'email', 'phone', 'cur', '<', '>'])\n",
    "\n",
    "words_before = [word[0] for word in top10000_before]\n",
    "freq_before = [freq[1] for freq in top10000_before]\n",
    "\n",
    "words_after = [word[0] for word in top10000_after]\n",
    "freq_after = [freq[1] for freq in top10000_after]\n",
    "\n",
    "# Bar plot of the top 10,000 most frequent words before and after preprocessing\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "ax1.plot(freq_before)\n",
    "ax1.set_xlabel(\"Word Index\")\n",
    "ax1.set_ylabel(\"Frequencies\")\n",
    "ax1.set_title(\"Before preprocessing\")\n",
    "\n",
    "ax2.plot(freq_after)\n",
    "ax2.set_xlabel(\"Word Index\")\n",
    "ax2.set_ylabel(\"Frequencies\")\n",
    "ax2.set_title(\"After preprocessing\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics and visualizations\n",
    "\n",
    "# Distribution of domains\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "domain_counts.plot(kind='bar')\n",
    "plt.xlabel(\"Domains\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Domain frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Calculates the average number of tokens before and after preprocessing\n",
    "df['content_length_before'] = df['tokens'].apply(len)\n",
    "df['content_length_after'] = df['tokens_stemmed'].apply(len)\n",
    "\n",
    "avg_before = np.mean(df['content_length_before'])\n",
    "avg_after = np.mean(df['content_length_after'])\n",
    "\n",
    "print(f\"Average number of tokens per article before preprocessing: {avg_before:.2f}\")\n",
    "print(f\"Average number of tokens per article after preprocessing: {avg_after:.2f}\")\n",
    "\n",
    "# Create a bar chart to visualize the difference\n",
    "plt.bar(['Before preprocessing', 'After preprocessing'], [avg_before, avg_after])\n",
    "plt.ylabel(\"Average number of tokens\")\n",
    "plt.title(\"Average content length before and after preprocessing\")\n",
    "plt.show()\n",
    "\n",
    "# Counts number of articles in each category\n",
    "\n",
    "type_counts = df['type'].value_counts()\n",
    "plt.figure(figsize=(12, 6))\n",
    "type_counts.plot(kind='bar')\n",
    "plt.xlabel(\"Article type\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of articles by type\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "# print(\"reading\")\n",
    "# data = pd.read_csv('news_sample.csv', engine=\"python\")\n",
    "\n",
    "# print(\"cleaning\")\n",
    "# data['content'] = data['content'].apply(clean_content)\n",
    "\n",
    "# print (\"tokenizing\")\n",
    "# # Tokenize the text\n",
    "# data['content'] = data['content'].apply(word_tokenize)\n",
    "\n",
    "# print (\"stopwords\")\n",
    "# # Remove stopwords\n",
    "# data['content'] = data['content'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# print(\"stemming\")\n",
    "# # Stemming\n",
    "# data['content'] = data['content'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "# print(\"to csv\")\n",
    "# data.to_csv('news_sample_cleaned2.csv', index=False)\n",
    "\n",
    "# Vi laver en ny fil med kun 2 content-kolonner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = pd.read_csv('news_sample_cleaned2.csv', nrows=10000)\n",
    "\n",
    "df_large.fillna('', inplace=True)\n",
    "print(df_large.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This is part of part 2 task 0\n",
    "df_large['label'] = df_large['type'].apply(lambda x: 'reliable' if x in ['reliable', 'political'] else 'fake')\n",
    "\n",
    "# This is part of part 2 task 1\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_large['content'])\n",
    "y = df_large['label']\n",
    "    \n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2, task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, C=10, penalty='l2')),\n",
    "    ('Naive Bayes', MultinomialNB(alpha=0.1)),\n",
    "    ('Support Vector Machine', LinearSVC(max_iter=1000, C=10, penalty='l2'))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the range of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create a grid search object and fit it to the training data\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", grid_search.best_score_)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Define the hyperparameters for the Naive Bayes classifier\n",
    "nb_params = {'alpha': [0.1, 0.5, 1.0]}\n",
    "\n",
    "# Create a Naive Bayes classifier with the hyperparameters\n",
    "nb = MultinomialNB()\n",
    "nb_grid_search = GridSearchCV(nb, nb_params, cv=5)\n",
    "nb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Naive Bayes best hyperparameters:\", nb_grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", nb_grid_search.best_score_)\n",
    "\n",
    "# Define the hyperparameters for the SVM classifier\n",
    "svm_params = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Create an SVM classifier with the hyperparameters\n",
    "svm = LinearSVC(max_iter=1000)\n",
    "svm_grid_search = GridSearchCV(svm, svm_params, cv=5)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"SVM best hyperparameters:\", svm_grid_search.best_params_)\n",
    "print(\"Validation accuracy:\", svm_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean title\n",
    "# print(\"cleaning\")\n",
    "# df_large['title'] = df_large['title'].apply(clean_content)\n",
    "\n",
    "# print(\"tokenizing\")\n",
    "# # Tokenize title\n",
    "# df_large['title'] = df_large['title'].apply(word_tokenize)\n",
    "\n",
    "# print(\"stopwords\")\n",
    "# # Remove stopwords from title\n",
    "# df_large['title'] = df_large['title'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "# print(\"stemming\")\n",
    "# # Stemming title\n",
    "# df_large['title'] = df_large['title'].apply(lambda tokens: [ss.stem(token) for token in tokens])\n",
    "\n",
    "# print(\"to csv\")\n",
    "# df_large['title'].to_csv('processed_titles.csv', index=False)\n",
    "\n",
    "df_large_titles = pd.read_csv('processed_titles.csv', nrows=10000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2, task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_title = vectorizer.fit_transform(df_large_titles['title'])\n",
    "X_domain = vectorizer.fit_transform(df_large['domain'])\n",
    "X_keywords = vectorizer.fit_transform(df_large['meta_keywords'])\n",
    "\n",
    "X_combined = hstack([X, X_keywords])\n",
    "\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "X_val_combined, X_test_combined, y_val_combined, y_test_combined = train_test_split(X_test_combined, y_test_combined, test_size=0.5, random_state=42)\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train_combined, y_train_combined)\n",
    "    y_pred_combined = model.predict(X_test_combined)\n",
    "    accuracy_combined = accuracy_score(y_test_combined, y_pred_combined)\n",
    "    print(f\"{name} accuracy (combined features): {accuracy_combined:.4f}\")\n",
    "# Den får 100% pga. artiklerne er klassificeret på baggrund af domænet og ikke artikler, overfitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def f1_score_custom(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "X_val_nn, X_test_nn, y_val_nn, y_test_nn = train_test_split(X_test_nn, y_test_nn, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy', f1_score_custom])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train.toarray(), y_train_nn, validation_data=(X_val.toarray(), y_val_nn), epochs=10, batch_size=32)\n",
    "\n",
    "# Test the model\n",
    "nnscore = model.evaluate(X_test.toarray(), y_test_nn, batch_size=32)\n",
    "print(f\"Neural network accuracy: {nnscore[1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('Support Vector Machine', LinearSVC(max_iter=1000))\n",
    "]\n",
    "# We show the confusion matrix and f1-score for each simple model\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} F1-score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4, task 1\n",
    "# We computed this value (nnscore) when completing part 3. We saved the result in a variable which we just use here\n",
    "\n",
    "print(f\"Neural network:\\nAccuracy: {nnscore[1]:.4f} \\nF1 score: {nnscore[2]:.4f} \")\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"\\n{name}: \\nAccuracy: {accuracy:.4f} \\nF1 score: {f1score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_liar_tsv(file_path):\n",
    "    columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state', 'party_affiliation',\n",
    "               'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "    return pd.read_csv(file_path, sep='\\t', header=None, names=columns, index_col=False)\n",
    "\n",
    "train_df = read_liar_tsv('train.tsv')\n",
    "test_df = read_liar_tsv('test.tsv')\n",
    "valid_df = read_liar_tsv('valid.tsv')\n",
    "\n",
    "liar_df = pd.concat([train_df, test_df, valid_df]).reset_index(drop=True)\n",
    "\n",
    "# Preprocess the LIAR dataset\n",
    "liar_df['label'] = liar_df['label'].apply(lambda x: 'reliable' if x in ['true', 'mostly-true', 'half-true'] else 'fake')\n",
    "\n",
    "X_liar = vectorizer.fit_transform(liar_df['statement'])\n",
    "y_liar = liar_df['label']\n",
    "\n",
    "\n",
    "y_liar_encoded = le.transform(y_liar)\n",
    "y_liar_categorical = to_categorical(y_liar_encoded)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for name, model in models:\n",
    "    y_liar_pred = model.predict(X_liar)\n",
    "    f1 = f1_score(y_liar, y_liar_pred, pos_label='reliable')\n",
    "    print(f\"{name} F1-score on LIAR dataset: {f1:.4f}\")\n",
    "\n",
    "# Evaluate the advanced model on the LIAR dataset\n",
    "nn_liar_score = model.evaluate(X_liar.toarray(), y_liar_categorical, batch_size=32)\n",
    "print(f\"Neural network F1-score on LIAR dataset: {nn_liar_score[2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
